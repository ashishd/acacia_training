{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c50f72-af4c-4303-a1b2-fd7e230a4e1a",
   "metadata": {},
   "source": [
    "# Using Acacia in HPC workflows\n",
    "\n",
    "Until now we have been using Rclone and MinIO client on the command line. You can certainly interact with Acacia using the command line on Pawsey systems, however there are more powerful ways to integrate object storage into your HPC workflows. Since the time limit for files on **/scratch** is 30 days, object storage on Acacia is one of the **primary means** for longer term data storage at Pawsey. In this tutorial we are going to look at:\n",
    "\n",
    "\n",
    "* Using **rclone** and **mc** on Pawsey systems\n",
    "* Using the Unix utility **tar** to create, examine, compress, and extract files in archive format.\n",
    "* Leveraging job dependencies to combine Acacia with your supercomputer workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bd323-c5d7-4ed3-92fd-667ccfa15a67",
   "metadata": {},
   "source": [
    "## Getting onto a Pawsey system\n",
    "\n",
    "For this tutorial you need to be using a Pawsey system. If you have not already logged in to a Pawsey system then please see the <a href=\"../T1_Getting_Access/L1_SSH_access.html\">L1_SSH_access</a> page for logging in via SSH. Then revisit the <a href=\"../T1_Getting_Access/L3_MinIO_client.html\">MinIO</a> and <a href=\"../T1_Getting_Access/L4_Rclone_client.html\">Rclone</a> setup pages to setup access to Acacia from MinIO and Rclone clients on Pawsey systems. In each page, go to the section called \"Configure (MinIO/Rclone) with your personal acccess key\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac1a5a9-87bc-4fc5-86b9-9f5a45f7464c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare the mock data\n",
    "\n",
    "If you haven't already prepared the mock data, then follow the instructions at  <a href=\"../T1_Getting_Access/L5_Mock_data.html\">T1_Getting_Access -> L5_Mock_data</a> to unpack the mock data for working with Acacia.\n",
    "\n",
    "Using the **command line** change directory to the **data** directory. This will be something like\n",
    "\n",
    "```bash\n",
    "cd /scratch/${PAWSEY_PROJECT}/${USER}/acacia_training/data\n",
    "```\n",
    "\n",
    "Replace **username** with your training login."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1869e6-842a-4ed4-8afa-90631e358b05",
   "metadata": {},
   "source": [
    "## A mini tutorial on tar \n",
    "\n",
    "TAR (Tape ARchive) is a Unix tool to sequentially aggregate many files and directories into **one** file. Tar's intended purpose was to prepare files for being written to tape, but the file it creates now serves as a general archive format that supports POSIX file metadata. A tar file may be compressed, and **tar** supports compression using **gzip** and **bzip2**. \n",
    "\n",
    "### When to use tar\n",
    "\n",
    "It might be useful to integrate tar into your workflow under the following circumstances:\n",
    "\n",
    "* When there are so many files that their storage in Acacia will exceed the nominal limit of 100,000 objects per bucket\n",
    "* When there is a performance benefit in aggregating a large number of small files. Each upload to Acacia is a unique https connection which takes time to set up and tear down.\n",
    "* When you don't need individual access to files.\n",
    "* When you need to preserve empty directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f2cf8-7a32-40b2-acf2-84ddc137cb87",
   "metadata": {},
   "source": [
    "### When to use compression with tar\n",
    "\n",
    "Tar files can be compressed as well. This is sometimes useful when the files benefit from being compressed. For many types of binary data the space saved through compression is often marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d3f32-e4f7-41c5-8e81-8f40fe0f1c9a",
   "metadata": {},
   "source": [
    "### Creating archives\n",
    "\n",
    "The basic syntax to create a tar archive is:\n",
    "\n",
    "```bash\n",
    "tar cf <output_name>.tar <things_to_include>\n",
    "```\n",
    "The **c** flag means **create** and the **f** (or **--file**) flag means use a **file** for output. For example to archive the **simulation** directory and all of the contents (including hidden files) run\n",
    "```bash\n",
    "tar cf simulation.tar simulation \n",
    "```\n",
    "or to include everything in the current directory, including hidden files, use this:\n",
    "```bash\n",
    "tar cf ../simulation.tar .\n",
    "```\n",
    "Usually it's better to put the tar file somewhere else other than the directory being archived, otherwise you get a warning about the tar file trying to include itself in the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046abfe-6f22-430a-98ca-8b5c4a8687b3",
   "metadata": {},
   "source": [
    "**Exercise: Run these commands on the simulation directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c798990-c349-4d76-a576-71b61b380de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb34420f-79a9-4f1a-b3ed-57ba0d39961b",
   "metadata": {},
   "source": [
    "#### Verbosity\n",
    "\n",
    "If you need to see extra information use the **v** or **--verbose** flag for verbosity. This generates a lot of output for a big directory! Change directory back to **data** and run this\n",
    "\n",
    "```bash\n",
    "cd ../data\n",
    "tar cvf simulation.tar simulation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901378f-1656-4d26-895e-957620721a85",
   "metadata": {},
   "source": [
    "#### Using compression to create an archive\n",
    "\n",
    "The **z** or **j** flag (but not both) switches on compression. The **j** flag corresponds to **bzip2** compression and the resulting file usually is given a **.tar.bz2** extension. The flag **z** is for gzip compression and the file has a **.tar.gz** extension. Normally bzip2 achieves a better compression ratio than gzip but is slower.\n",
    "\n",
    "```bash\n",
    "tar zcf simulation.tar.gz simulation\n",
    "```\n",
    "or \n",
    "```bash\n",
    "tar jcf simulation.tar.bz2 simulation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22f9f6-76d5-452c-9a35-00b957a3d9bc",
   "metadata": {},
   "source": [
    "**Exercise: Use the \"time\" utility to see how long the tar archive creation process takes without compression.**\n",
    "\n",
    "```bash\n",
    "time tar cf simulation.tar simulation\n",
    "```\n",
    "\n",
    "Now try timing archive creation using the two compression options. \n",
    "\n",
    "* Which process took the longest? \n",
    "* How many times longer did it take than uncompressed? \n",
    "* Which process achieved the smallest size? Use **du -h \\<filename\\>** to look at the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eec032-1682-4237-83e0-7c1087a7965a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f16170d8-5034-41ff-924b-91ab3349f2c2",
   "metadata": {},
   "source": [
    "#### Compression in parallel\n",
    "\n",
    "The standard tar with compression only uses one core, however there is a program called **pigz** (Parallel Implementation of GZip) which can use multiple cores, thus speeding up the compress and decompress component. We can enable this program using the --use-compress-program=\"pigz\"\n",
    "\n",
    "```bash\n",
    "time tar cf simulation.tar.gz simulation --use-compress-program=\"pigz\"\n",
    "```\n",
    "and to uncompress we use\n",
    "\n",
    "```bash\n",
    "time tar xf simulation.tar.gz --use-compress-program=\"pigz\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df440a7-6d75-41bb-9fb2-1790c16912b0",
   "metadata": {},
   "source": [
    "#### Exclude files\n",
    "\n",
    "Files can be left out of the archive using one or more **--exclude** flags. Here we exclude all files ending with **.dat**. The **exclude** flag must come after the tar file name **simulation.tar**.\n",
    "\n",
    "```bash\n",
    "tar cvf simulation.tar --exclude='*.dat' simulation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0599cb-b6a9-4347-a812-8fcd7171c224",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Listing archive contents\n",
    "\n",
    "The **--list** or **t** flag shows what is in an archive. You can even use this on a compressed archive by using it in conjunction with a compression flag.\n",
    "\n",
    "```bash\n",
    "tar tf simulation.tar\n",
    "```\n",
    "or for a tar file that is compressed with bzip2 we do this.\n",
    "```bash\n",
    "tar tjf simulation.tar.bz2\n",
    "```\n",
    "\n",
    "There is even a rudimentary search facility using wildcards. Here we use it to look for log files in the archive.\n",
    "```bash\n",
    "tar tf simulation.tar --wildcards '*.log'\n",
    "tar tf simulation.tar --wildcards 'simulation/log/data_0*.log'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dfba5-1fed-443d-a413-b52a0cc2f02c",
   "metadata": {},
   "source": [
    "**Exercise: list the contents of simulation.tar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d3b6c-ecc8-4722-beed-b7361869b1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f725a373-1ca0-477b-86e1-a75c18733edc",
   "metadata": {},
   "source": [
    "### Extracting archives\n",
    "\n",
    "The **x** flag is used to extract files from an archive\n",
    "\n",
    "```bash\n",
    "tar xvf simulation.tar\n",
    "```\n",
    "You can extract the contents of a tar archive to another directory using this flag\n",
    "```bash\n",
    "tar xvf simulation.tar --directory other_directory\n",
    "```\n",
    "Compressed volumes are extracted using their respective compression flags. Use **j** for a bzip2 compressed archive and **z** for a gzip compressed archive.\n",
    "```bash\n",
    "tar xjvf simulation.tar.bz2\n",
    "```\n",
    "You can also extract a single file or directory from an archive, for example just the **log** directory and everything in it.\n",
    "```bash\n",
    "tar xvf simulation.tar simulation/log\n",
    "```\n",
    "Or you can use wildcards to extract specific things based on regular expression style patterns.\n",
    "```bash\n",
    "tar xvf simulation.tar --wildcards '*.log'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff6323-b7db-42c6-890e-8fe656fdf160",
   "metadata": {},
   "source": [
    "**Exercise: extract all of the .dat files from the simulation.tar archive and place them in another directory of your choosing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d3dee-edd2-46d3-93f4-68b3f8047c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "492f9752-413f-4225-8fd8-922e00e2d278",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Adding files to an archive\n",
    "\n",
    "Duplicate the **simulation** directory\n",
    "\n",
    "```bash\n",
    "cp -r simulation simulation2\n",
    "```\n",
    "\n",
    "Adding files to a tar archive is accomplished using the **--append** or **r** flag. Run this to add the directory **simulation2** to the archive.\n",
    "\n",
    "```bash\n",
    "tar rf simulation.tar simulation2\n",
    "```\n",
    "\n",
    "Now list the contents of simulation.tar and check the size\n",
    "\n",
    "```bash\n",
    "tar tf simulation.tar\n",
    "du -h simulation.tar\n",
    "```\n",
    "\n",
    "> Note: compressed tar files **cannot** be updated. You need to unpack the compressed file before appending more contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d426-c3d6-4c02-af11-6fe68d975ee5",
   "metadata": {},
   "source": [
    "### Removing files from an archive\n",
    "\n",
    "Now remove the **simulation2** directory from the tar archive using the **--delete** flag. There is no short form of the delete flag. \n",
    "\n",
    "```bash\n",
    "tar --delete --file=simulation.tar simulation2\n",
    "```\n",
    "Note that the size of simulation.tar decreases accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19c6b2-4cf6-4317-a5b4-339770222f26",
   "metadata": {},
   "source": [
    "### Comparing files in an archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21780e67-8014-4c1e-9d27-aaebd4e3c8af",
   "metadata": {},
   "source": [
    "The **--compare**, **--diff**, or **-d** flag checks an archive to see if there are any differences betwen what is on the archive and what is on disk. Comparison is **not a recursive operation** though. Let's delete a file from the archive and add an extra file to the local copy.\n",
    "\n",
    "```bash\n",
    "tar --delete --file=simulation.tar simulation/results/data_00.dat\n",
    "cp simulation/results/data_00.dat simulation/results/data_100.dat\n",
    "```\n",
    "Now compare the archive to what is on disk\n",
    "```bash\n",
    "tar --compare --file=simulation.tar simulation/results/data*.dat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e79340-a76b-477c-82b8-68b5b3dafd5f",
   "metadata": {},
   "source": [
    "### Streaming a tar archive to and from Acacia\n",
    "\n",
    "Sometimes it can be problematic and/or slow to create an intermediate tar file. On Pawsey systems it is possible to stream the output from **tar** directly to Acacia using the **rclone rcat** command. I wouldn't recommend doing this for extremely large or mission-critical transfers though. Nor would I recommend this method for transferring data to Acacia from outside Pawsey. For those transactions you need the checksumming abilities in **rclone** and **mc**.\n",
    "\n",
    "#### Streaming tar files to Acacia with progress\n",
    "\n",
    "A single hyphen (-) instead of a filename tells **tar** to use standard output (input) as the destination (source). \n",
    "\n",
    "```bash\n",
    "tar cf - simulation | rclone rcat acacia-mine:$BucketName/simulation.tar --progress\n",
    "```\n",
    "or tar with multicore compression\n",
    "```bash\n",
    "tar cf - simulation --use-compress-program=\"pigz\" | rclone rcat acacia-mine:$BucketName/simulation.tar.gz --progress\n",
    "```\n",
    "\n",
    "Remove the **--progress** flag when using these commands in scripts, replace **--progress** with **-q** or **--quiet** to suppress unnecessary output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fb183-d6ce-4165-91f5-cf159b833a54",
   "metadata": {},
   "source": [
    "#### Streaming from the archive\n",
    "\n",
    "When streaming tar files from Acacia you can use the **cat** command. Don't use it with the **--progress** flag though!\n",
    "\n",
    "```bash\n",
    "rclone cat acacia-mine:$BucketName/simulation.tar | tar xf - \n",
    "```\n",
    "or with multicore de-compression to another directory\n",
    "```bash\n",
    "mkdir -p simulation2\n",
    "rclone cat acacia-mine:$BucketName/simulation.tar.gz | tar xf - --directory=simulation2/ --use-compress-program=\"pigz\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6533ac5-c767-4b74-8ece-cbf5493b3182",
   "metadata": {},
   "source": [
    "### Chunk limits and maximum file sizes\n",
    "\n",
    "When streaming to an S3 backend such as Acacia, the uploads are chunked with a hard limit of 10,000 chunks. By default for **rclone** and **mc** each chunk size is 5MiB, giving a maximum tar file size of just under 50GiB. You can increase the chunk size for Rclone by adding the line\n",
    "\n",
    "```bash\n",
    "chunk_size = 1G\n",
    "```\n",
    "to each remote in ~/.config/rclone/rclone.conf. Each entry should have the **chunk_size** flag added, like this:\n",
    "\n",
    "```bash\n",
    "[acacia-mine]\n",
    "type = s3\n",
    "provider = Ceph\n",
    "access_key_id = <Personal Access ID>\n",
    "secret_access_key = <Personal Access Key>\n",
    "endpoint = https://projects.pawsey.org.au\n",
    "chunk_size = 1G\n",
    "```\n",
    "That will increase the maximum allowed tar file size to just under 10TiB.\n",
    "\n",
    "MinIO client also has options for increasing the chunk size. I haven't been able to find out which config file switch to use. \n",
    "\n",
    "> From hpc-data I was able to upload 50GB of files to Acacia using tar streaming at a rate of around 133MB/s over nearly 6 minutes. Please note there are **no integrity checks** with this streaming method. For comparison **rclone sync** with option **--transfers 12** was able to sync this same directory of files at an average rate of 671 MB/s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e3466-fbf7-4cea-ba70-41a488c94de0",
   "metadata": {},
   "source": [
    "## Integrating object storage in Pawsey job submission scripts\n",
    "\n",
    "Since **/scratch** is not for long term access then it can be advantageous to incorporate Acacia into Pawsey supercomputer workflows. Acacia can be both the **origin** and **destination** for data that is processed on a Pawsey supercomputer by your job scripts. Data is **staged** in from Acacia to scratch, a **compute** job is run, then processed data is **stored** back to Acacia where it can be accessed from anywhere.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:80%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/hpc_workflow.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: HPC workflows with Acacia.</figcaption>\n",
    "</figure>\n",
    "\n",
    "There are important pieces of information to make this work.\n",
    "\n",
    "* Job dependencies\n",
    "* Use of the copy queues\n",
    "\n",
    "### Leveraging job dependencies\n",
    "\n",
    "A typical supercomputing job script might look something like this, variables in curly brackets {} you would normally substitute with information specific to you.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash --login\n",
    "#SBATCH --account={account}\n",
    "#SBATCH --partition={work queue}\n",
    "#SBATCH --job-name=superJob\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node={number of mpi tasks per node}\n",
    "#SBATCH --cpus-per-task={number of openmp threads per mpi task}\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --export=NONE\n",
    "\n",
    "module load sometool\n",
    "\n",
    "srun mkdir -p {scratchDir}\n",
    "cd {scratchDir}\n",
    "\n",
    "srun toolName arguments\n",
    "```\n",
    "\n",
    "We call this script **superscript.sh**. It normally run like this:\n",
    "\n",
    "```bash\n",
    "sbatch superscript.sh\n",
    "```\n",
    "and produces the **Job ID** (i.e 5554515) in the output at job submission.\n",
    "\n",
    "```bash\n",
    "Submitted batch job 5554515\n",
    "```\n",
    "The **--parsable** flag just renders sbatch output with the job ID and cluster name (if defined)\n",
    "```bash\n",
    "5554515\n",
    "```\n",
    "\n",
    "If you capture the **Job ID** you can use it as a dependency for another script. Here we submit a script to stage data from Acacia and capture the Job ID using the **cut** command.\n",
    "\n",
    "```bash\n",
    "stageJobID=$(sbatch --parsable stagescript.sh | cut -d ' ' -f 1)\n",
    "```\n",
    "\n",
    "Now we can use that JobID to submit a job that uses the flags **--dependency=afterok:$JobID --parsable** to **wait** for the staging script to finish before starting. \n",
    "\n",
    "> **You must specify the **--dependency** flags **before** the name of the job script when running sbatch!**\n",
    "\n",
    "```bash\n",
    "superJobID=$(sbatch --dependency=afterok:$stageJobID --parsable superscript.sh  | cut -d ' ' -f 1)\n",
    "```\n",
    "\n",
    "Job dependencies are the key to leveraging Acacia with your Pawsey jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2c655-606c-4553-a58a-68f184345f7f",
   "metadata": {},
   "source": [
    "### Stage and store scripts\n",
    "\n",
    "Both stage and store scripts don't usually use much CPU resources. If you submit them to the **copy** queue it **won't count** towards your allocation. A typical staging script (stagescript.sh) might look like this. Notice the use of the **copy** queue and low CPU counts in the request.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash --login\n",
    "#SBATCH --account={account}\n",
    "#SBATCH --job-name=stageTar\n",
    "#SBATCH --partition={copy_queue}\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --export=NONE\n",
    "\n",
    "module load rclone/{rclone_version}\n",
    "\n",
    "# Stage files from Acacia\n",
    "\n",
    "# Streaming approach\n",
    "mkdir -p {scratchDir}\n",
    "srun rclone cat {acaciaAlias}:{acaciaInPath} | tar xf - --directory {scratchDir}/ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae5f83-37a2-4bc1-bbbe-5cedc5531379",
   "metadata": {},
   "source": [
    "A typical storage script (storescript.sh) might look like this:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash --login\n",
    "#SBATCH --account={account}\n",
    "#SBATCH --job-name=storeTar\n",
    "#SBATCH --partition {copy_queue}\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --export=NONE\n",
    "\n",
    "module load rclone/{rclone_version}\n",
    "\n",
    "# Store files to Acacia\n",
    "\n",
    "# Streaming approach\n",
    "cd {scratchDir}\n",
    "srun tar cf - . | rclone rcat {acaciaAlias}:{acaciaOutPath}\n",
    "```\n",
    "\n",
    "Of course you can put any data movement technique that you prefer into the **stage** and **store** scripts. For the store script we must also depend on the supercomputing job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb301d-372a-4936-b88f-409d1f6d108b",
   "metadata": {},
   "source": [
    "```bash\n",
    "storeJobID=$(sbatch --dependency=afterok:$superJobID --parsable storescript.sh  | cut -d ' ' -f 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86735c79-889b-43ea-826e-9a96c262c9fd",
   "metadata": {},
   "source": [
    "### Master workflow script\n",
    "\n",
    "All of these steps can be combined into one master control script\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Master script to control your Pawsey workflow\n",
    "\n",
    "# Run the staging script, there is no prior dependency\n",
    "stageJobID=$(sbatch stagescript.sh | cut -d ' ' -f 1)\n",
    "\n",
    "# Now the supercomputing script, depend on staging\n",
    "superJobID=$(sbatch --dependency=afterok:$stageJobID --parsable superscript.sh | cut -d ' ' -f 1)\n",
    "\n",
    "# And the storage script, depend on supercomputing\n",
    "storeJobID=$(sbatch --dependency=afterok:$superJobID --parsable storescript.sh | cut -d ' ' -f 1)\n",
    "\n",
    "# Check the queues to see how we are going\n",
    "squeue --me\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9cec0-6f1a-465d-b8f3-076101510682",
   "metadata": {},
   "source": [
    "## Exercise 1: Enable a trial workflow using job dependencies\n",
    "\n",
    "In this exercise we are going to use the mock data as part of an exercise in using job dependencies in a Pawsey workflow. We are going to stage data from Acacia, run a supercomputer job, and store data back to Acacia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c83f8-8c2b-4d47-a803-53f385e45fd9",
   "metadata": {},
   "source": [
    "### Preparation of input tar file\n",
    "\n",
    "We first need data on Acacia. Use **cd** to change directory to the **data** directory of the course material folder and unzip the **data.zip** file if the **simulation** directory is not present.\n",
    "\n",
    "```bash\n",
    "cd data\n",
    "# If simulation is not present\n",
    "unzip data.zip\n",
    "```\n",
    "Assuming rclone is set up, we **tar** the simulation directory and upload to Acacia as follows: \n",
    "\n",
    "```bash\n",
    "tar zcf - simulation | rclone rcat acacia-mine:$BucketName/simulation.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ddeb0-3b3a-461e-bbbf-0c50b176af5f",
   "metadata": {},
   "source": [
    "### Editing scripts\n",
    "\n",
    "In the **scripts** directory of the course material folder are four scripts to accomplish this:\n",
    "\n",
    "* stagescript.sh\n",
    "* superscript.sh\n",
    "* storescript.sh\n",
    "* masterWorkflow.sh\n",
    "\n",
    "The script **stagescript.sh** is responsible for staging data from Acacia, **superscript.sh** is a fake supercomputer job, and **storescript.sh** stores the data back to Acacia. Your task is to edit these scripts, replacing everything in curly brackets {} with meaningful values, so that the master workflow script **masterWorkflow.sh** is able to run the scripts without issue. You will need to supply the following values while editing the scripts.\n",
    "\n",
    "* {account} - your project name/ID.\n",
    "* {acaciaAlias} - alias to access Acacia.\n",
    "* {acaciaInPath} - path (on Acacia) of the input tar file, e.g. <bucket_name\\>/simulation.tar.gz (with no leading forward slash!)\n",
    "* {acaciaOutPath} - path (on Acacia) of the tar file to use as output e.g. <bucket_name\\>/simulation_out.tar.gz\n",
    "* {copy_queue} - The queue to use for copying files. On Setonix it is **copy**, on other Pawsey systems it is **copyq**.\n",
    "* {debug_queue} - The queue to use for the supercomputer job. On Setonix it is **debug**, on other Pawsey systems it is **debugq**.\n",
    "* {rclone_version} run the command **module spider rclone** to see which version is available.\n",
    "* {scratchDir} - path (on /scratch) of the directory where the incoming tar file will be unpacked, e.g. /scratch/${PAWSEY_PROJECT}/username/working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b6917-a4e7-42e3-80a3-e9dc39361902",
   "metadata": {},
   "source": [
    "### Running the scripts\n",
    "\n",
    "Now that you've edited the scripts in the **scripts** directory you can make them executable and go ahead and run them.\n",
    "\n",
    "```bash\n",
    "chmod u+x *.sh\n",
    "./masterWorkflow.sh\n",
    "```\n",
    "\n",
    "Success means you see output similar to the following, there must be the text **Dependency** in the REASON section for **storeTar** and superJob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cf1ae-4397-4267-b0ae-5a9eb19e6eab",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "JOBID    USER     ACCOUNT     PARTITION            NAME EXEC_HOST ST     REASON   START_TIME     END_TIME  TIME_LEFT NODES   PRIORITY\n",
    "5555102  <username>  <account>   copyq            storeTar       n/a PD Dependency          N/A          N/A    1:00:00     1      75325\n",
    "5555101  <username>  <account>   debugq           superJob       n/a PD Dependency          N/A          N/A       1:00     1      75325\n",
    "5555100  <username>  <account>   copyq            stageTar       n/a PD   Priority          N/A          N/A    1:00:00     1      75325\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a106e41-a5a1-413f-80a7-87e667a3c134",
   "metadata": {},
   "source": [
    "List the contents of your bucket to make sure data is being copied there from the job\n",
    "\n",
    "```bash\n",
    "rclone ls acacia-mine:\n",
    "```\n",
    "\n",
    "By all means download the tar file that is created, and examine the contents to make sure everything is there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed184183-cf3d-4f10-abc2-890d77008077",
   "metadata": {},
   "source": [
    "## Exercise 2: Slightly more advanced scripting (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0de80-4b78-4fdc-b4d7-8da90a99feb7",
   "metadata": {},
   "source": [
    "Keeping track of the same variables over four scripts can be prone to error. It is more robust to define variables once. The job submission command **sbatch** has the ability to read a script from standard input, so we can compose job scripts as multi-line strings and feed them directly to **sbatch** without creating separate files. In the **scripts** directory are two files:\n",
    "\n",
    "* advancedWorkflow.sh\n",
    "* advancedWorkflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e931e9-54bf-4eee-bee2-50a5313c1af4",
   "metadata": {},
   "source": [
    "Each performs the same task as the four scripts in the previous exercise, however they use either [Bash Heredocs](https://tldp.org/LDP/abs/html/here-docs.html) or [Python f-strings](https://realpython.com/python-f-strings/) to compose the **stage**, **super**, and **store** job scripts as multiline strings. You only need to edit variables at the start of the script. There is some extra functionality included in these scripts that might be useful for your workflows.\n",
    "\n",
    "* If **acaciaInPath** is not set then the stage script is not queued\n",
    "* If **runSuper** is not set then the super script is not queued\n",
    "* If **acaciaOutPath** is not set then the store script is not queued\n",
    "* Steps in the workflow will attempt to depend on previous ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4aa26f-8e8e-4314-84c9-6dda13141d9d",
   "metadata": {},
   "source": [
    "### Prepare the scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23203c-f7da-4c32-8b0e-0da37589eaf6",
   "metadata": {},
   "source": [
    "Choose a script to run (or both) and edit to set variables **account, acaciaAlias, acaciaInPath, scratchDir, and acaciaOutPath** as before. Examine the code where the job scripts are created, and see how these variables are included dynamically into the variables **stageScript**, **superScript**, and **storeScript**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62c892-70ef-4254-8828-bf9877dd393a",
   "metadata": {},
   "source": [
    "### Run the scripts\n",
    "\n",
    "To run the script **advancedWorkflow.sh** do the following:\n",
    "\n",
    "```bash\n",
    "chmod u+x advancedWorkflow.sh\n",
    "./advancedWorkflow.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c75d7-e0f7-41ec-825f-81a98f56b011",
   "metadata": {},
   "source": [
    "The Python script **advancedWorkflow.py** needs the Python 3 module loaded\n",
    "\n",
    "```bash\n",
    "chmod u+x advancedWorkflow.py\n",
    "module load python/<version>\n",
    "./advancedWorkflow.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c93811-73b9-498d-b6fe-3ee4f846ef23",
   "metadata": {},
   "source": [
    "Success means you see something like the following output. Note the **Dependency** text in the REASON field.\n",
    "\n",
    "```bash\n",
    "JOBID        USER ACCOUNT                   NAME EXEC_HOST ST     REASON START_TIME       END_TIME  TIME_LEFT NODES   PRIORITY\n",
    "290501     <username> <account>             storeTar       n/a PD Dependency N/A                   N/A    1:00:00     1      75355\n",
    "290500     <username> <account>             superJob       n/a PD Dependency N/A                   N/A       1:00     1      75355\n",
    "290499     <username> <account>             stageTar       n/a PD   Priority N/A                   N/A    1:00:00     1      75355\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897353fb-b54e-4c91-8a56-742d747ae80f",
   "metadata": {},
   "source": [
    "As before, list the contents of your bucket to make sure data is being copied there from the job\n",
    "\n",
    "```bash\n",
    "rclone ls acacia-mine:\n",
    "```\n",
    "\n",
    "By all means download the tar file that is created, and examine the contents to make sure everything is there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc8139-3990-4c77-aa11-bd971ffa3b14",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you got this far then congratulations! You have successfully ran a workflow that includes Acacia as part of your supercomputing jobs. In this lesson we looked at different tar techniques and how they can work to support file uploads to Acacia. Then we looked at ways of integrating these methods into your Pawsey workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
